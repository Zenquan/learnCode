**1	搭建环境**

部署节点操作系统为CentOS，防火墙和SElinux禁用，创建了一个shiyanlou用户并在系统根目录下创建/app目录，用于存放Hadoop等组件运行包。因为该目录用于安装hadoop等组件程序，用户对shiyanlou必须赋予rwx权限（一般做法是root用户在根目录下创建/app目录，并修改该目录拥有者为shiyanlou(chown –R shiyanlou:shiyanlou /app）。

**Hadoop搭建环境：**

- 虚拟机操作系统： CentOS6.6  64位，单核，1G内存
- JDK：1.7.0_55 64位
- Hadoop：1.1.2

**2	Sqoop介绍**

**2.1	Sqoop简介**

Sqoop即 SQL to Hadoop ，是一款方便的在传统型数据库与Hadoop之间进行数据迁移的工具，充分利用MapReduce并行特点以批处理的方式加快数据传输，发展至今主要演化了二大版本，Sqoop1和Sqoop2。  

Sqoop工具是hadoop下连接关系型数据库和Hadoop的桥梁，支持关系型数据库和hive、hdfs，hbase之间数据的相互导入，可以使用全表导入和增量导入。 
那么为什么选择Sqoop呢？  
- 高效可控的利用资源，任务并行度，超时时间。  
- 数据类型映射与转化，可自动进行，用户也可自定义  
- 支持多种主流数据库，MySQL,Oracle，SQL Server，DB2等等  

**2.2	Sqoop1和Sqoop2比较**

**2.2.1	Sqoop1和Sqoop2异同**

- 两个不同的版本，完全不兼容  
- 版本号划分区别，Apache版本：1.4.x(Sqoop1); 1.99.x(Sqoop2)     CDH版本 : Sqoop-1.4.3-cdh4(Sqoop1) ; Sqoop2-1.99.2-cdh4.5.0 (Sqoop2) 
- Sqoop2比Sqoop1的改进  
 1. 引入Sqoop server，集中化管理connector等  
 2. 多种访问方式：CLI,Web UI，REST API  
 3. 引入基于角色的安全机制

**2.2.2	Sqoop1与Sqoop2的架构图 **

Sqoop架构图1 
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988408517.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 

Sqoop架构图2 
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988417189.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.2.3	Sqoop1与Sqoop2的优缺点**

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988431084.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

**3	安装部署Sqoop **

**3.1	下载Sqoop**

可以到apache基金sqoop官网http://hive.apache.org/，选择镜像下载地址：http://mirror.bit.edu.cn/apache/sqoop/下载一个稳定版本，如下图所示下载支持Hadoop1.X的1.4.5版本gz包：
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988448561.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
也可以在/home/shiyanlou/install-pack目录中找到该安装包，解压该安装包并把该安装包复制到/app目录中
- cd /home/shiyanlou/install-pack
- tar -xzf sqoop-1.4.5.bin__hadoop-1.0.0.tar.gz
- mv sqoop-1.4.5.bin__hadoop-1.0.0 /app/sqoop-1.4.5
- ll /app

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988455155.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**3.2	设置/etc/profile参数**

编辑/etc/profile文件，加入sqoop的Home路径和在PATH加入bin的路径：
- export SQOOP_HOME=/app/sqoop-1.4.5
- export PATH=$PATH:$SQOOP_HOME/bin

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988482354.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

编译配置文件/etc/profile，并确认生效
- source /etc/profile
- echo $PATH

**3.3	设置bin/configure-sqoop配置文件**

修改bin/configure-sqoop配置文件 
- cd /app/sqoop-1.4.5/bin
- sudo vi configure-sqoop

注释掉HBase和Zookeeper等检查（除非使用HBase和Zookeeper等HADOOP上的组件）
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988509547.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**3.4	设置conf/sqoop-env.sh配置文件**

如果不存在sqoop-env.sh文件，复制sqoop-env-template.sh文件，然后修改sqoop-env.sh配置文件
- cd /app/sqoop-1.4.5/conf
- cp sqoop-env-template.sh sqoop-env.sh
- sudo vi sqoop-env.sh

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988528498.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 
设置hadoop运行程序所在路径和hadoop-*-core.jar路径（Hadoop1.X需要配置）
- export HADOOP_COMMON_HOME=/app/hadoop-1.1.2
- export HADOOP_MAPRED_HOME=/app/hadoop-1.1.2

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988537162.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 
编译配置文件sqoop-env.sh使之生效

**3.5	验证安装完成**

输入如下命令验证是否正确安装sqoop，如果正确安装则出现sqoop提示
- sqoop help

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988551752.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 

**4	文件导入/导出 **

**4.1	MySql数据导入到HDFS中**

如果没有安装MySql，请参照第8课3.1进行安装

**4.1.1	下载MySql驱动**

到MySql官网进入下载页面：http://dev.mysql.com/downloads/connector/j/ ，选择所需要的版本进行下载，这里下载的zip格式的文件，然后在本地解压：
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988568882.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
也可以在/home/shiyanlou/install-pack目录中找到该安装包，把MySql驱动包使用如下命令放到Sqoop的lib目录下
- cd /home/shiyanlou/install-pack
- cp mysql-connector-java-5.1.22-bin.jar /app/sqoop-1.4.5/lib

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988581378.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.1.2	启动MySql服务**

查看MySql服务并查看状态，如果没有启动则启动服务
- sudo service mysql status
- sudo service mysql start

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988593162.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.1.3	查看MySql中的数据表**

进入MySql数据库，选择有数据的一张表查看内容，比较导出结果是否正确，输入如下命令：
- mysql -uhive -phive
- mysql>show databases;
- mysql>use hive;
- mysql>show tables;
- mysql>select TBL_ID, CREATE_TIME, DB_ID, OWNER, TBL_NAME,TBL_TYPE from TBLS;

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988631637.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988644070.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.1.4	把MySql数据导入到HDFS中**

使用如下命令列出MySql中所有数据库：
- sqoop list-databases --connect jdbc:mysql://hadoop:3306/ --username hive --password hive

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988655926.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
使用如下命令把hive数据库TBLS表数据导入到HDFS中：
- sqoop import --connect jdbc:mysql://hadoop:3306/hive --username hive --password hive --table TBLS -m 1
 1. --username 数据库用户名 
 2. --password连接数据库密码
 3. --table 表名 
 4. -m 1表示map数

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988667183.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.1.5	查看导出结果**

使用如下命令查看导出到HDFS结果，文件路径在当前用户hadoop目录下增加了TBLS表目录，查看part-m-00000文件：
- hadoop fs -ls /user/shiyanlou/TBLS
- hadoop fs -cat /user/shiyanlou/TBLS/part-m-00000

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988678851.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.2	MySql数据导入到Hive中**

**4.2.1	启动metastore和hiveserver**

在使用hive之前需要启动metastore和hiveserver服务，通过如下命令启用：
- hive --service metastore &
- hive --service hiveserver &

启动用通过jps命令可以看到两个进行运行在后台

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988725412.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.2.2	从MySql导入表数据到Hive中**

使用如下命令到把MySql中TBLS表数据导入到Hive中：
- sqoop import --connect jdbc:mysql://hadoop:3306/hive --username hive --password hive --table TBLS --hive-table MySql2Hive --hive-import -m 1
 1. -- username为mysql中的数据库连接用户名
 2. --password为mysql中的数据库连接密码
 3. --table为导出表
 4. --hive-table test1 为导出表在Hive中的名称
 5. -m 1表示map数

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988744843.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988753664.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 
 
从运行的日志可以看到，这个过程有两个阶段：
1. 第一个阶段是从MySql中把数据到HDFS文件中
2. 第二个阶段是从HDFS中把数据写入到MySql中

**4.2.3	查看导出结果**

登录hive，在hive创建表并查看该表，命令如下：
- hive
- hive>show tables;
- hive>desc MySql2Hive;
 
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1045timestamp1433988775534.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
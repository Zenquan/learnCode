**1	搭建环境**

部署节点操作系统为CentOS，防火墙和SElinux禁用，创建了一个shiyanlou用户并在系统根目录下创建/app目录，用于存放Hadoop等组件运行包。因为该目录用于安装hadoop等组件程序，用户对shiyanlou必须赋予rwx权限（一般做法是root用户在根目录下创建/app目录，并修改该目录拥有者为shiyanlou(chown –R shiyanlou:shiyanlou /app）。

**Hadoop搭建环境：**

- 虚拟机操作系统： CentOS6.6  64位，单核，1G内存
- JDK：1.7.0_55 64位
- Hadoop：1.1.2

**2	Mahout介绍**

Mahout 是 Apache Software Foundation（ASF） 旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。AMahout包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。

Mahout的意思是大象的饲养者及驱赶者。Mahout 这个名称来源于该项目（有时）使用 Apache Hadoop —其徽标上有一头黄色的大象 —来实现可伸缩性和容错性。

Mahout 项目是由 Apache Lucene（开源搜索）社区中对机器学习感兴趣的一些成员发起的，他们希望建立一个可靠、文档翔实、可伸缩的项目，在其中实现一些常见的用于集群和分类的机器学习算法。该社区最初基于 Ng et al. 的文章 “Map-Reduce for Machine Learning on Multicore”（见 参考资料），但此后在发展中又并入了更多广泛的机器学习方法。Mahout 的目标还包括：
- 建立一个用户和贡献者社区，使代码不必依赖于特定贡献者的参与或任何特定公司和大学的资金。
- 专注于实际用例，这与高新技术研究及未经验证的技巧相反。
- 提供高质量文章和示例

**3	搭建Mahout环境**

**3.1	部署过程**

**3.1.1	下载Mahout**

在Apache下载最新的Mahout软件包，点击下载会推荐最快的镜像站点，以下为下载地址：http://archive.apache.org/dist/mahout/0.6/
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945535873.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
也可以在/home/shiyanlou/install-pack目录中找到该安装包，解压该安装包并把该安装包复制到/app目录中
- cd /home/shiyanlou/install-pack
- tar -xzf mahout-distribution-0.6.tar.gz
- mv mahout-distribution-0.6 /app/mahout-0.6

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945551031.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**3.1.2	设置环境变量**

使用如下命令编辑/etc/profile文件：
- sudo vi /etc/profile

声明mahout的home路径和在path加入bin的路径：
- export MAHOUT_HOME=/app/mahout-0.6
- export MAHOUT_CONF_DIR=/app/mahout-0.6/conf
- export PATH=$PATH:$MAHOUT_HOME/bin

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945560908.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
编译配置文件/etc/profile，并确认生效
- source /etc/profile
- echo $PATH

**3.1.3	验证安装完成**

重新登录终端，确保hadoop集群启动，键入mahout --help命令，检查Mahout是否安装完好，看是否列出了一些算法：
- mahout --help

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945581044.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**3.2	测试例子**

**3.2.1	下载测试数据**

下载一个文件synthetic_control.data，下载地址:

```
http://labfile.oss.aliyuncs.com/courses/237/synthetic_control.data  
```

把这个文件放在$MAHOUT_HOME/testdata目录下

- cd /home/shiyanlou/install-pack/class9
- wget 
- mkdir /app/mahout-0.6/testdata
- mv synthetic_control.data /app/mahout-0.6/testdata

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945594571.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**3.2.2	启动Hadoop**

通过下面命令启动hadoop并通过jps查看进程
- cd /app/hadoop-1.1.2/bin
- ./start-all.sh
- jps

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945603546.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**3.2.3	使用kmeans算法**

使用如下命令进行kmeans算法测试：
- cd /app/mahout-0.6/
- mahout org.apache.mahout.clustering.syntheticcontrol.kmeans.Job

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945614782.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
这里需要说明下，当你看到下面的代码时以为是错的，其实不是，原因：MAHOUT_LOCAL：设置是否本地运行，如果设置该参数就不会在hadoop运行了，一旦设置这个参数那HADOOP_CONF_DIR 和HADOOP_HOME两个参数就自动失效了。

```
MAHOUT_LOCAL is not set, so we don't add HADOOP_CONF_DIR to classpath.
no HADOOP_HOME set , running locally
```

**3.2.4	查看结果**

结果会在根目录建立output新文件夹，如果下图结果表示mahout安装正确且运行正常：
- cd /app/mahout-0.6/output
- ll

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945629059.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4	测试例子：运行20newsgroup**

**4.1	算法流程**

朴素贝叶斯分类是一种十分简单的分类算法，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率哪个最大，就认为此待分类项属于哪个类别。

这二十个新闻组数据集合是收集大约20,000新闻组文档，均匀的分布在20个不同的集合。这20个新闻组集合采集最近流行的数据集合到文本程序中作为实验，根据机器学习技术。例如文本分类，文本聚集。我们将使用Mahout的Bayes Classifier创造一个模型，它将一个新文档分类到这20个新闻组集合范例演示
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945643881.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 

**4.2	实现过程（mahout 0.6版本）**

**4.2.1	下载数据并解压数据**

下载20Newsgroups数据集，地址为 http://qwone.com/~jason/20Newsgroups/ ，下载20news-bydate.tar.gz数据包，也可以在/home/shiyanlou/install-pack/class9目录中找到该测试数据文件：
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945670391.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
解压20news-bydate.tar.gz数据包，解压后可以看到两个文件夹，分别为训练原始数据和测试原始数据：
- cd /home/shiyanlou/install-pack/class9
- tar -xzf 20news-bydate.tar.gz

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945683513.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
在mahout根目录下建data文件夹，然后把20news训练原始数据和测试原始数据迁移到该文件夹下：
- mkdir /app/mahout-0.6/data
- mv 20news-bydate-t* /app/mahout-0.6/data
- ll /app/mahout-0.6/data

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945695833.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.2.2	建立训练集**

通过如下命令建立训练集，训练的数据在20news-bydate-train目录中，输出的训练集目录为 bayes-train-input：
- cd /app/mahout-0.6
- mahout org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups \
- -p /app/mahout-0.6/data/20news-bydate-train \
- -o /app/mahout-0.6/data/bayes-train-input \
- -a org.apache.mahout.vectorizer.DefaultAnalyzer \
- -c UTF-8

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945718229.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.2.3	建立测试集**

通过如下命令建立训练集，训练的数据在20news-bydate-test目录中，输出的训练集目录为 bayes-test-input：
- mahout org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups \
- -p /app/mahout-0.6/data/20news-bydate-test \
- -o /app/mahout-0.6/data/bayes-test-input \
- -a org.apache.mahout.vectorizer.DefaultAnalyzer \
- -c UTF-8

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945728940.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.2.4	上传数据到HDFS**

在HDFS中新建/class9/20news文件夹，把生成的训练集和测试集上传到HDFS的/class9/20news目录中：
- hadoop fs -mkdir /class9/20news
- hadoop fs -put /app/mahout-0.6/data/bayes-train-input /class9/20news
- hadoop fs -put /app/mahout-0.6/data/bayes-test-input /class9/20news
- hadoop fs -ls /class9/20news
- hadoop fs -ls /class9/20news/bayes-test-input

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945741182.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

**4.2.5	训练贝叶斯分类器**

使用trainclassifier类训练在HDFS中/class9/20news/bayes-train-input的数据，生成的模型放到/class9/ 20news/newsmodel 目录中：
- mahout trainclassifier \
- -i /class9/20news/bayes-train-input \
- -o /class9/20news/newsmodel \
- -type cbayes \
- -ng 2 \
- -source hdfs

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945766159.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.2.6	观察训练作业运行过程**

注：实验楼为命令行界面，无法观测到该步骤界面，以下描述仅做参考
在训练过程中在JobTracker页面观察运行情况，链接地址为http://**.***.**.***:50030/jobtracker.jsp，训练任务四个作业，大概运行了15分钟左右：
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945792073.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 
点击查看具体作业信息
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945807326.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
map运行情况
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945814007.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 
作业运行情况
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945866063.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.2.7	查看生成模型**

通过如下命令查看模型内容：
- hadoop fs -ls /class9/20news
- hadoop fs -ls /class9/20news/newsmodel
- hadoop fs -ls /class9/20news/newsmodel/trainer-tfIdf

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945878234.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.2.8	测试贝叶斯分类器**

使用testclassifier类训练在HDFS中./20news/bayestest-input的数据，使用的模型路径为./ 20news/newsmodel：
- mahout testclassifier \
- -m /class9/20news/newsmodel \
- -d /class9/20news/bayes-test-input \
- -type cbayes \
- -ng 2 \
- -source hdfs \
- -method mapreduce

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945889995.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.2.9	观察训练作业运行过程**

注：实验楼为命令行界面，无法观测到该步骤界面，以下描述仅做参考
在执行过程中在JobTracker页面观察运行情况，链接地址为http://hadoop:50030/jobtracker.jsp，训练任务1个作业，大概运行了5分钟左右：
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945904847.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 
作业的基本信息
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945919517.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 
map运行情况
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945925270.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 
reduce运行情况
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945947731.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**4.2.10	查看结果**

这个混合矩阵的意思说明：上述a到u分别是代表了有20类别，这就是我们之前给的20个输入文件，列中的数据说明每个类别中被分配到的字节个数，classified说明应该被分配到的总数
381  0  0  0  0  9  1  0  0  0  1  0  0  2  0  1  0  0  3  0  0  |  398  a = rec.motorcycles
意思为rec.motorcycles 本来是属于 a，有381篇文档被划为了a类，这个是正确的数据，其它的分别表示划到 b~u类中的数目。我们可以看到其正确率为 381/398=0.9573 ,可见其正确率还是很高的了。
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945966747.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

 
**4.3	实现过程（mahout 0.7+版本）**

在0.7版本的安装目录下$MAHOUT_HOME/examples/bin下有个脚本文件classifu-20newsgroups.sh，这个脚本中执行过程是和前面分布执行结果是一致的，只不过将各个API用shell脚本封装到一起了。从0.7版本开始，Mahout移除了命令行调用的API：prepare20newsgroups、trainclassifier和testclassifier，只能通过shell脚本执行。
执行 $MAHOUT_HOME/examples/bin/classify-20newsgroups.sh 四个选项中选择第一个选项，
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945982636.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 
执行结果如下：
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid29778labid1043timestamp1433945992830.png?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**5	问题解决**

**5.1	使用mahout0.7+版本对20Newsgroup数据建立训练集时出错**

使用如下命令对20Newsgroupt数据建立训练集时：
- mahout org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups \
- -p /app/mahout-0.9/data/20news-bydate-train \
- -o /app/mahout-0.9/data/bayes-train-input \
- -a org.apache.mahout.vectorizer.DefaultAnalyzer\
- -c UTF-8

出现如下错误，原因在于从0.7版本开始，Mahout移除了命令行调用的prepare20newsgroups、trainclassifier和testclassifier API，只能通过shell脚本执行$MAHOUT_HOME/examples/bin/classify-20newsgroups.sh进行

```
14/12/7 21:31:35 WARN driver.MahoutDriver: Unable to add class: org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups
14/12/7 21:31:35 WARN driver.MahoutDriver: No org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups.props found on classpath, will use command-line arguments only
Unknown program 'org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups' chosen.
Valid program names are:
  arff.vector: : Generate Vectors from an ARFF file or directory
  baumwelch: : Baum-Welch algorithm for unsupervised HMM training
  .......
```